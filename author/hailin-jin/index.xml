<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hailin Jin | Minghang Zheng&#39;s homepage</title>
    <link>https://minghangz.github.io/author/hailin-jin/</link>
      <atom:link href="https://minghangz.github.io/author/hailin-jin/index.xml" rel="self" type="application/rss+xml" />
    <description>Hailin Jin</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© 2025 Minghang Zheng</copyright><lastBuildDate>Mon, 10 Jul 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://minghangz.github.io/media/icon_hub259b9e287cb654bd255d2333b0dd2cd_21941_512x512_fill_lanczos_center_3.png</url>
      <title>Hailin Jin</title>
      <link>https://minghangz.github.io/author/hailin-jin/</link>
    </image>
    
    <item>
      <title>Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization</title>
      <link>https://minghangz.github.io/publication/spl/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/spl/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div&gt;
&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-IlFc2IH-E0&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{zheng-etal-2023-generating,
    title = &amp;quot;Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization&amp;quot;,
    author = &amp;quot;Zheng, Minghang  and
      Gong, Shaogang  and
      Jin, Hailin  and
      Peng, Yuxin  and
      Liu, Yang&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&amp;quot;,
    month = jul,
    year = &amp;quot;2023&amp;quot;,
    address = &amp;quot;Toronto, Canada&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2023.acl-long.794&amp;quot;,
    pages = &amp;quot;14197--14209&amp;quot;,
    abstract = &amp;quot;Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found at https://github.com/minghangz/SPL.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work was supported by the grants from the Zhejiang Lab (NO.2022NB0AB05), National Natural Science Foundation of China (61925201,62132001,U22B2048), CAAI-Huawei MindSpore Open Fund, Alan Turing Institute Turing Fellowship, Veritone and Adobe. We thank MindSpore for the partial support of this work, which is a new deep learning computing framework.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
