[{"authors":null,"categories":null,"content":"I am currently a PHD student in Wangxuan Institute of Computer Technology, Peking University, supervised by Prof. Yang Liu). I received my bachelor degree from Peking University in June 2022. My research interests include computer vision, multimodal learning, etc.\n","date":1752969600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752969600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://minghangz.github.io/author/minghang-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/minghang-zheng/","section":"authors","summary":"I am currently a PHD student in Wangxuan Institute of Computer Technology, Peking University, supervised by Prof. Yang Liu). I received my bachelor degree from Peking University in June 2022. My research interests include computer vision, multimodal learning, etc.","tags":null,"title":"Minghang Zheng","type":"authors"},{"authors":["Minghang Zheng","Puxin Peng","Benyuan Sun","Yi Yang","Yang Liu"],"categories":null,"content":"Video \r\rCitation @inproceedings{zheng-etal-2025-hierarchical,\rtitle = \u0026quot;Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding\u0026quot;,\rauthor = \u0026quot;Zheng, Minghang and\rPeng, Yuxin and\rSun, Benyuan and\rYang, Yi and\rLiu, Yang\u0026quot;,\rbooktitle = \u0026quot;Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\u0026quot;,\ryear = \u0026quot;2025\u0026quot;\r}\r ","date":1752969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752969600,"objectID":"93a2859ba374d82547ad0469fdd3c027","permalink":"https://minghangz.github.io/publication/onvtg/","publishdate":"2025-07-20T00:00:00Z","relpermalink":"/publication/onvtg/","section":"publication","summary":"In this paper, we tackle the task of online video temporal grounding (OnVTG), which requires the model to locate events related to a given natural language query within a video stream. Unlike regular video temporal grounding, OnVTG requires the model to make predictions without observing future frames. As online videos are streaming inputs and can go on indefinitely, it is impractical and inefficient to store all historical inputs. The existing OnVTG model employs memory to store recent historical video frame features and predict scores indicating whether the current frame corresponds to the start or end time of the target event. However, these methods lack effective event modeling and cannot retain long-term historical information, leading to low performance. To tackle these challenges, we propose a hierarchical event memory for online video temporal grounding. We propose an event-based OnVTG framework that makes predictions based on event proposals that model event-level information with various durations. To efficiently preserve historically valuable event information, we introduce a hierarchical event memory that retains long-term historical events, allowing the model to access both recent fine-grained information and long-term coarse-grained information. To enable the real-time prediction of the start time, we further propose a future prediction branch that predicts whether the target event will occur in the near future and further regresses the start time of the event. We achieve state-of-the-art performance on the ActivityNet Captions, TACoS, and MAD datasets. Code is available at https://github.com/minghangz/OnVTG.","tags":null,"title":"Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding","type":"publication"},{"authors":["Minghang Zheng","Jiahua Zhang","Qingchao Chen","Yuxin Peng","Yang Liu"],"categories":null,"content":"Video \r\rCitation @inproceedings{zheng-etal-2024-training,\rtitle = \u0026quot;ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding\u0026quot;,\rauthor = \u0026quot;Zheng, Minghang and Zhang, Jiahua and Chen, Qingchao and Peng, Yuxin and Liu, Yang\u0026quot;,\rbooktitle = \u0026quot;Proceedings of the 31st ACM International Conference on Multimedia\u0026quot;,\ryear = \u0026quot;2024\u0026quot;\r}\r Acknowledgement This work was supported by grants from the National Natural Science Foundation of China (62372014, 61925201, 62132001, U22B2048).\n","date":1725148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725148800,"objectID":"62f4b8c0144a078c07ac36c38209ad0d","permalink":"https://minghangz.github.io/publication/resvg/","publishdate":"2024-09-01T00:00:00Z","relpermalink":"/publication/resvg/","section":"publication","summary":"Visual grounding aims to localize the object referred to in an image based on a natural language query. Although progress has been made recently, accurately localizing target objects within multiple-instance distractions (multiple objects of the same category as the target) remains a significant challenge. Existing methods demonstrate a significant performance drop when there are multiple distractions in an image, indicating an insufficient understanding of the fine-grained semantics and spatial relationships between objects. In this paper, we propose a novel approach, the \\textbf{Re}lation and \\textbf{S}emantic-sensitive \\textbf{V}isual \\textbf{G}rounding (ResVG) model, to address this issue. Firstly, we enhance the model's understanding of fine-grained semantics by injecting semantic prior information derived from text queries into the model. This is achieved by leveraging text-to-image generation models to produce images representing the semantic attributes of target objects described in queries. Secondly, we tackle the lack of training samples with multiple distractions by introducing a relation-sensitive data augmentation method. This method generates additional training data by synthesizing images containing multiple objects of the same category and pseudo queries based on their spatial relationships. The proposed ReSVG model significantly improves the model's ability to comprehend both object semantics and spatial relations, leading to enhanced performance in visual grounding tasks, particularly in scenarios with multiple-instance distractions. We conduct extensive experiments to validate the effectiveness of our methods on five datasets. Code is available at https://github.com/minghangz/ResVG.","tags":null,"title":"ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding","type":"publication"},{"authors":["Minghang Zheng","Xinhao Cai","Qingchao Chen","Yuxin Peng","Yang Liu"],"categories":null,"content":"Video \r\rCitation @inproceedings{zheng-etal-2024-training,\rtitle = \u0026quot;Training-free Video Temporal Grounding usingLarge-scale Pre-trained Models\u0026quot;,\rauthor = \u0026quot;Zheng, Minghang and\rCai, Xinhao and\rChen, Qingchao and\rPeng, Yuxin and\rLiu, Yang\u0026quot;,\rbooktitle = \u0026quot;Proceedings of the European Conference on Computer Vision (ECCV)\u0026quot;,\ryear = \u0026quot;2024\u0026quot;\r}\r Acknowledgement This work was supported by grants from the National Natural Science Foundation of China (62372014, 61925201, 62132001, U22B2048).\n","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722470400,"objectID":"63620dc54a2e5766b2828c60631a5930","permalink":"https://minghangz.github.io/publication/tfvtg/","publishdate":"2024-08-01T00:00:00Z","relpermalink":"/publication/tfvtg/","section":"publication","summary":"Video temporal grounding aims to identify video segments within untrimmed videos that are most relevant to a given natural language query. Existing video temporal localization models rely on specific datasets for training, with high data collection costs, but exhibit poor generalization capability under the across-dataset and out-of-distribution (OOD) settings. In this paper, we propose a Training-Free Video Temporal Grounding (TFVTG) approach that leverages the ability of pre-trained large models. A naive baseline is to enumerate proposals in the video and use the pre-trained visual language models (VLMs) to select the best proposal according to the vision-language alignment. However, most existing VLMs are trained on image-text pairs or trimmed video clip-text pairs, making it struggle to (1) grasp the relationship and distinguish the temporal boundaries of multiple events within the same video; (2) comprehend and be sensitive to the dynamic transition of events (the transition from one event to another) in the video. To address these issues, firstly, we propose leveraging large language models (LLMs) to analyze multiple sub-events contained in the query text and analyze the temporal order and relationships between these events. Secondly, we split a sub-event into dynamic transition and static status parts and propose the dynamic and static scoring functions using VLMs to better evaluate the relevance between the event and the description. Finally, for each sub-event description provided by LLMs, we use VLMs to locate the top-k proposals that are most relevant to the description and leverage the order and relationships between sub-events provided by LLMs to filter and integrate these proposals.  Our method achieves the best performance on zero-shot video temporal grounding on Charades-STA and ActivityNet Captions datasets without any training and demonstrates better generalization capabilities in cross-dataset and OOD settings. Code is available at https://github.com/minghangz/TFVTG.","tags":null,"title":"Training-free Video Temporal Grounding usingLarge-scale Pre-trained Models","type":"publication"},{"authors":["Minghang Zheng","Shaogang Gong","Hailin Jin","Yuxin Peng","Yang Liu"],"categories":null,"content":"Video \r\r\r\r\rCitation @inproceedings{zheng-etal-2023-generating,\rtitle = \u0026quot;Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization\u0026quot;,\rauthor = \u0026quot;Zheng, Minghang and\rGong, Shaogang and\rJin, Hailin and\rPeng, Yuxin and\rLiu, Yang\u0026quot;,\rbooktitle = \u0026quot;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\u0026quot;,\rmonth = jul,\ryear = \u0026quot;2023\u0026quot;,\raddress = \u0026quot;Toronto, Canada\u0026quot;,\rpublisher = \u0026quot;Association for Computational Linguistics\u0026quot;,\rurl = \u0026quot;https://aclanthology.org/2023.acl-long.794\u0026quot;,\rpages = \u0026quot;14197--14209\u0026quot;,\rabstract = \u0026quot;Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found at https://github.com/minghangz/SPL.\u0026quot;,\r}\r Acknowledgement This work was supported by the grants from the Zhejiang Lab (NO.2022NB0AB05), National Natural Science Foundation of China (61925201,62132001,U22B2048), CAAI-Huawei MindSpore Open Fund, Alan Turing Institute Turing Fellowship, Veritone and Adobe. We thank MindSpore for the partial support of this work, which is a new deep learning computing framework.\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"143c4f3cea42cb957fd110ebe2ef666f","permalink":"https://minghangz.github.io/publication/spl/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/publication/spl/","section":"publication","summary":"Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found at https://github.com/minghangz/SPL.","tags":null,"title":"Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization","type":"publication"},{"authors":["Minghang Zheng","Sizhe Li","Qingchao Chen","Yuxin Peng","Yang Liu"],"categories":null,"content":"Video \r\r\r\r\rCitation @article{Zheng_Li_Chen_Peng_Liu_2023,\rtitle = {Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization},\rvolume = {37},\rurl = {https://ojs.aaai.org/index.php/AAAI/article/view/25478},\rdoi = {10.1609/aaai.v37i3.25478},\rabstractnote = {In this paper, we address the problem of video temporal sentence localization, which aims to localize a target moment from videos according to a given language query. We observe that existing models suffer from a sheer performance drop when dealing with simple phrases contained in the sentence. It reveals the limitation that existing models only capture the annotation bias of the datasets but lack sufficient understanding of the semantic phrases in the query. To address this problem, we propose a phrase-level Temporal Relationship Mining (TRM) framework employing the temporal relationship relevant to the phrase and the whole sentence to have a better understanding of each semantic entity in the sentence. Specifically, we use phrase-level predictions to refine the sentence-level prediction, and use Multiple Instance Learning to improve the quality of phrase-level predictions. We also exploit the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. The proposed approach sheds light on how machines can understand detailed phrases in a sentence and their compositions in their generality rather than learning the annotation biases. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. Code can be found at https://github.com/minghangz/TRM.},\rnumber = {3},\rjournal = {Proceedings of the AAAI Conference on Artificial Intelligence},\rauthor = {Zheng, Minghang and Li, Sizhe and Chen, Qingchao and Peng, Yuxin and Liu, Yang},\ryear = {2023},\rmonth = {Jun.},\rpages = {3669-3677}\r}\r Acknowledgement This work was supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025, 62201014), Zhejiang Lab (NO.2022NB0AB05), the National Key R\u0026amp;D Program of China (2021YFF0901502) and CAAI-Huawei MindSpore Open Fund.\n","date":1675728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675728000,"objectID":"8fe3558e2d10c8bdbd5fdf98cacd3ceb","permalink":"https://minghangz.github.io/publication/trm/","publishdate":"2023-02-07T00:00:00Z","relpermalink":"/publication/trm/","section":"publication","summary":"In this paper, we address the problem of video temporal sentence localization, which aims to localize a target moment from videos according to a given language query. We observe that existing models suffer from a sheer performance drop when dealing with simple phrases contained in the sentence. It reveals the limitation that existing models only capture the annotation bias of the datasets but lack sufficient understanding of the semantic phrases in the query. To address this problem, we propose a phrase-level Temporal Relationship Mining (TRM) framework employing the temporal relationship relevant to the phrase and the whole sentence to have a better understanding of each semantic entity in the sentence. Specifically, we use phrase-level predictions to refine the sentence-level prediction, and use Multiple Instance Learning to improve the quality of phrase-level predictions. We also exploit the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. The proposed approach sheds light on how machines can understand detailed phrases in a sentence and their compositions in their generality rather than learning the annotation biases. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. Code can be found at https://github.com/minghangz/TRM.","tags":["oral"],"title":"Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization","type":"publication"},{"authors":["Minghang Zheng","Yanjie Huang","Qingchao Chen","Yuxin Peng","Yang Liu"],"categories":null,"content":"Citation @inproceedings{CPL_2022_CVPR,\rtitle = {Weakly Supervised Temporal Sentence Grounding with Gaussian-based Contrastive Proposal Learning},\rauthor = {Zheng, Minghang and Huang, Yanjie and Chen, Qingchao and Peng, Yuxin and Liu, Yang},\rbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\ryear = {2022}\r}  Acknowledgement This work is supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025) and Zhejiang Lab (NO.2022NB0AB05).\n","date":1648512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648512000,"objectID":"0d7645820edcb2d6687e661fe9431419","permalink":"https://minghangz.github.io/publication/cpl/","publishdate":"2022-03-29T00:00:00Z","relpermalink":"/publication/cpl/","section":"publication","summary":"Temporal sentence grounding aims to detect the most salient moment corresponding to the natural language query from untrimmed videos. As labeling the temporal boundaries is labor-intensive and subjective, the weakly-supervised methods have recently received increasing attention. Most of the existing weakly-supervised methods generate the proposals by sliding windows, which are content-independent and of low quality. Moreover, they train their model to distinguish positive visual-language pairs from negative ones randomly collected from other videos, ignoring the highly confusing video segments within the same video. In this paper, we propose Contrastive Proposal Learning(CPL) to overcome the above limitations. Specifically, we use multiple learnable Gaussian functions to generate both positive and negative proposals within the same video that can characterize the multiple events in a long video. Then, we propose a controllable easy to hard negative proposal mining strategy to collect negative samples within the same video, which can ease the model optimization and enables CPL to distinguish highly confusing scenes. The experiments show that our method achieves state-of-the-art performance on Charades-STA and ActivityNet Captions datasets. The code and models are available at this [https url](https://github.com/minghangz/cpl).","tags":null,"title":"Weakly Supervised Temporal Sentence Grounding with Gaussian-based Contrastive Proposal Learning","type":"publication"},{"authors":["Minghang Zheng","Yanjie Huang","Qingchao Chen","Yang Liu"],"categories":null,"content":"Citation @inproceedings{CNM_2022_AAAI,\rtitle = {Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining},\rauthor = {Zheng, Minghang and Huang, Yanjie and Chen, Qingchao and Liu, Yang},\rbooktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\ryear = {2022}\r}  Acknowledgement This work is supported by Zhejiang Lab (NO.2022NB0AB05), State Key Laboratory of Media Convergence Production Technology and Systems, National Engineering Laboratory for Big Data Analysis and Applications Technology. The authors would also like to thank Jiabo Huang and Sizhe Li for helpful suggestions.\n","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"35e429e4fff8d3817811fede1ed52ba6","permalink":"https://minghangz.github.io/publication/cnm/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/publication/cnm/","section":"publication","summary":"Video moment localization aims at localizing the video segments which are most related to the given free-form natural language query. The weakly supervised setting, where only video level description is available during training, is getting more and more attention due to its lower annotation cost. Prior weakly supervised methods mainly use sliding windows to generate temporal proposals, which are independent of video content and low quality, and train the model to distinguish matched video-query pairs and unmatched ones collected from different videos, while neglecting what the model needs is to distinguish the unaligned segments within the video. In this work, we propose a novel weakly supervised solution by introducing Contrastive Negative sample Mining (CNM). Specifically, we use a learnable Gaussian mask to generate positive samples, highlighting the video frames most related to the query, and consider other frames of the video and the whole video as easy and hard negative samples respectively. We then train our network with the Intra-Video Contrastive loss to make our positive and negative samples more discriminative. Our method has two advantages: (1) Our proposal generation process with a learnable Gaussian mask is more efficient and makes our positive sample higher quality. (2) The more difficult intra-video negative samples enable our model to distinguish highly confusing scenes. Experiments on two datasets show the effectiveness of our method. Code can be found at this [https url](https://github.com/minghangz/cnm).","tags":null,"title":"Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining","type":"publication"},{"authors":["Minghang Zheng","Peng Gao","Renrui Zhang","Kunchang Li","Hongsheng Li","Hao Dong"],"categories":null,"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"532903dc60d5c78af6956567a8622658","permalink":"https://minghangz.github.io/publication/act/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/publication/act/","section":"publication","summary":"End-to-end Object Detection with Transformer (DETR) performs object detection with Transformer and achieves comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial inputs. In this paper, a novel variant of transformer named Adaptive Clustering Transformer (ACT) has been proposed to reduce the computation cost for high-resolution input. ACT clusters the query features adaptively using Locality Sensitive Hashing (LSH) and approximates the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O($N^2$) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification. Code is released at this [https url](https://github.com/gaopengcuhk/SMCA-DETR/tree/main/Adaptive_Cluster_Transformer).","tags":["oral"],"title":"End-to-End Object Detection with Adaptive Clustering Transformer","type":"publication"},{"authors":["Peng Gao","Minghang Zheng","Xiaogang Wang","Jifeng Dai","Honghsneg Li"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"5fabf511230d1c95a3aec76358240de9","permalink":"https://minghangz.github.io/publication/smca/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/publication/smca/","section":"publication","summary":"The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate SMCA. Code is released at this [https url](https://github.com/gaopengcuhk/SMCA-DETR).","tags":null,"title":"Fast Convergence of DETR with Spatially Modulated Co-Attention","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://minghangz.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]