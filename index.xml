<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Minghang Zheng&#39;s homepage</title>
    <link>https://minghangz.github.io/</link>
      <atom:link href="https://minghangz.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Minghang Zheng&#39;s homepage</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© 2025 Minghang Zheng</copyright><lastBuildDate>Sun, 20 Jul 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://minghangz.github.io/media/icon_hub259b9e287cb654bd255d2333b0dd2cd_21941_512x512_fill_lanczos_center_3.png</url>
      <title>Minghang Zheng&#39;s homepage</title>
      <link>https://minghangz.github.io/</link>
    </image>
    
    <item>
      <title>Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding</title>
      <link>https://minghangz.github.io/publication/onvtg/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/onvtg/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;&#34;&gt;
    &lt;iframe src=&#34;https://www.youtube.com/embed/xQdlHzqECEM?si=Zs2xrWxGe_53j7lb&#34; 
            title=&#34;YouTube video player&#34; 
            frameborder=&#34;0&#34; 
            allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
            referrerpolicy=&#34;strict-origin-when-cross-origin&#34; 
            allowfullscreen
            style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;
    &lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{zheng-etal-2025-hierarchical,
    title = &amp;quot;Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding&amp;quot;,
    author = &amp;quot;Zheng, Minghang  and
      Peng, Yuxin  and
      Sun, Benyuan and
      Yang, Yi and
      Liu, Yang&amp;quot;,
    booktitle = &amp;quot;Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)&amp;quot;,
    year = &amp;quot;2025&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding</title>
      <link>https://minghangz.github.io/publication/resvg/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/resvg/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;&#34;&gt;
    &lt;iframe src=&#34;https://www.youtube.com/embed/Mtxkklph16o?si=DmfpRi8Y-jh2PRR7&#34; 
            title=&#34;YouTube video player&#34; 
            frameborder=&#34;0&#34; 
            allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
            referrerpolicy=&#34;strict-origin-when-cross-origin&#34; 
            allowfullscreen
            style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;
    &lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{zheng-etal-2024-training,
    title = &amp;quot;ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding&amp;quot;,
    author = &amp;quot;Zheng, Minghang and Zhang, Jiahua and Chen, Qingchao and Peng, Yuxin and Liu, Yang&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 31st ACM International Conference on Multimedia&amp;quot;,
    year = &amp;quot;2024&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work was supported by grants from the National Natural Science Foundation of China (62372014, 61925201, 62132001, U22B2048).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Training-free Video Temporal Grounding usingLarge-scale Pre-trained Models</title>
      <link>https://minghangz.github.io/publication/tfvtg/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/tfvtg/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;&#34;&gt;
    &lt;iframe src=&#34;https://www.youtube.com/embed/TQ6GBhwzRhg?si=CliwWuOBq2gp7N98&#34; 
            title=&#34;YouTube video player&#34; 
            frameborder=&#34;0&#34; 
            allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; 
            referrerpolicy=&#34;strict-origin-when-cross-origin&#34; 
            allowfullscreen
            style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;
    &lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{zheng-etal-2024-training,
    title = &amp;quot;Training-free Video Temporal Grounding usingLarge-scale Pre-trained Models&amp;quot;,
    author = &amp;quot;Zheng, Minghang  and
      Cai, Xinhao  and
      Chen, Qingchao  and
      Peng, Yuxin  and
      Liu, Yang&amp;quot;,
    booktitle = &amp;quot;Proceedings of the European Conference on Computer Vision (ECCV)&amp;quot;,
    year = &amp;quot;2024&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work was supported by grants from the National Natural Science Foundation of China (62372014, 61925201, 62132001, U22B2048).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization</title>
      <link>https://minghangz.github.io/publication/spl/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/spl/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div&gt;
&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-IlFc2IH-E0&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{zheng-etal-2023-generating,
    title = &amp;quot;Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization&amp;quot;,
    author = &amp;quot;Zheng, Minghang  and
      Gong, Shaogang  and
      Jin, Hailin  and
      Peng, Yuxin  and
      Liu, Yang&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&amp;quot;,
    month = jul,
    year = &amp;quot;2023&amp;quot;,
    address = &amp;quot;Toronto, Canada&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://aclanthology.org/2023.acl-long.794&amp;quot;,
    pages = &amp;quot;14197--14209&amp;quot;,
    abstract = &amp;quot;Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found at https://github.com/minghangz/SPL.&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work was supported by the grants from the Zhejiang Lab (NO.2022NB0AB05), National Natural Science Foundation of China (61925201,62132001,U22B2048), CAAI-Huawei MindSpore Open Fund, Alan Turing Institute Turing Fellowship, Veritone and Adobe. We thank MindSpore for the partial support of this work, which is a new deep learning computing framework.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization</title>
      <link>https://minghangz.github.io/publication/trm/</link>
      <pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/trm/</guid>
      <description>&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div&gt;
&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/sb4ixVYBVtg&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@article{Zheng_Li_Chen_Peng_Liu_2023,
  title        = {Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization},
  volume       = {37},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/25478},
  doi          = {10.1609/aaai.v37i3.25478},
  abstractnote = {In this paper, we address the problem of video temporal sentence localization, which aims to localize a target moment from videos according to a given language query. We observe that existing models suffer from a sheer performance drop when dealing with simple phrases contained in the sentence. It reveals the limitation that existing models only capture the annotation bias of the datasets but lack sufficient understanding of the semantic phrases in the query. To address this problem, we propose a phrase-level Temporal Relationship Mining (TRM) framework employing the temporal relationship relevant to the phrase and the whole sentence to have a better understanding of each semantic entity in the sentence. Specifically, we use phrase-level predictions to refine the sentence-level prediction, and use Multiple Instance Learning to improve the quality of phrase-level predictions. We also exploit the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. The proposed approach sheds light on how machines can understand detailed phrases in a sentence and their compositions in their generality rather than learning the annotation biases. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. Code can be found at https://github.com/minghangz/TRM.},
  number       = {3},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Zheng, Minghang and Li, Sizhe and Chen, Qingchao and Peng, Yuxin and Liu, Yang},
  year         = {2023},
  month        = {Jun.},
  pages        = {3669-3677}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work was supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025, 62201014), Zhejiang Lab (NO.2022NB0AB05), the National Key R&amp;amp;D Program of China (2021YFF0901502) and CAAI-Huawei MindSpore Open Fund.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Temporal Sentence Grounding with Gaussian-based Contrastive Proposal Learning</title>
      <link>https://minghangz.github.io/publication/cpl/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/cpl/</guid>
      <description>&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{CPL_2022_CVPR,
    title = {Weakly Supervised Temporal Sentence Grounding with Gaussian-based Contrastive Proposal Learning},
    author = {Zheng, Minghang and Huang, Yanjie and Chen, Qingchao and Peng, Yuxin and Liu, Yang},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2022}
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work is supported by the grants from the National Natural Science Foundation of China (61925201, 62132001, U21B2025) and Zhejiang Lab (NO.2022NB0AB05).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining</title>
      <link>https://minghangz.github.io/publication/cnm/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/cnm/</guid>
      <description>&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{CNM_2022_AAAI,
    title = {Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining},
    author = {Zheng, Minghang and Huang, Yanjie and Chen, Qingchao and Liu, Yang},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    year = {2022}
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;This work is supported by Zhejiang Lab (NO.2022NB0AB05), State Key Laboratory of Media Convergence Production Technology and Systems, National Engineering Laboratory for Big Data Analysis and Applications Technology. The authors would also like to thank Jiabo Huang and Sizhe Li for helpful suggestions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>End-to-End Object Detection with Adaptive Clustering Transformer</title>
      <link>https://minghangz.github.io/publication/act/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/act/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fast Convergence of DETR with Spatially Modulated Co-Attention</title>
      <link>https://minghangz.github.io/publication/smca/</link>
      <pubDate>Mon, 11 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/publication/smca/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://minghangz.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://minghangz.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
